---
layout: post
title: Review of Causal model in RL
date: 2021-10-18 14:37:00
categories: 强化学习
tags:  Cuasal-RL Review
mathjax: true

---

* content
{:toc}

### [Causal Confusion in Imitation Learning]()

本文强调/引入了一个概念：知道得越多，可能表现得越差（ **access to more information can yield worse performance**）。直观地说，数据集分布的偏移（distributional shift）可能会导致模型学到动作（Action）和一些无关的噪音之间的联系，而无法学到其和真正的因子（causal parent）之间的联系。也就是说，当我们想要得到一个对分布偏移现象表现稳定的策略时，我们做决策时必须仅仅依赖那些真正的因子。

为了更好地说明这个现象，该文设计了一个符号附加在观测图像上的特殊环境，实验表明此时需要更多地样本才能学到在原先环境一样的性能。

**定义**: 
分布偏移可以定义为对一个图中某些点（$X$）的干预（intervention），记受影响的点为$A$，那么我们关注的结果就是$p(A|do(X))$.

为了解决因果误识（causal misspecification）的问题，这篇文章提出：
(1) 为每一种可能的因果结构$G$（这里只考虑观测是动作的因子，故有$2^n$种可能，$n$是观测的维度）都学一套策略：$\pi_G(X)=f_\phi(X,G)$，优化目标就是最小化：
$$
\mathbb{E}_G [\ell (f_\phi(X_i\odot G,G),A_i)]
$$

(2) 计算似然概率 $\mathcal{L}(G)$。此时分两种情况： 
1. 可以询问专家：
![image-1](\images\2021-10-18-review of causal model RL\algo-1.png)
2. 不能询问专家：
![image-1](\images\2021-10-18-review of causal model RL\algo-2.png)

**笔者的疑问**: 

(1) 如何能用线性回归学$w$？ $w$ 是一个$2^n$维向量还是$n$维向量？

(2) 为什么 $f_\phi$ 是在$\mathbb{E}_G$下学的，而不是一个给定的$G$?


### [Causal Reasoning from Meta-reinforcement Learning]()

本文探索了用元强化学习辅助因果推理的可能性。

因果推理有两种：因果效用推理（causal-effect reasoning）和反事实推理（counterfactual reasoning）。前者指在去掉混淆因子的干扰下，正确地计算出因子对结果的效用/影响力；后者指在当前的因果结构和已发生的事实下，估计做另一种决策后会发生的事实。这两种推理都要求智能体能做到一些额外的事情，比如能够干预（即强行修改观测中的某些值，并得到环境的反馈，类似于RL中常说的reset）。因此，本文还设计了三种数据场景来实现因果推理：

1. 观测场景：智能体只能被动地拿到数据。在该场景下，智能体可以做相关性推理和给定因果结构下（即已知因果结构）的因果推理。
2. 干预场景：智能体可以对数据做干预并得到干预后的结果。此时可以做因果效用推理。
3. 反事实场景：智能体可以通过执行干预来学到正确的因果结构。此时可以利用观察到的已发生事实和因果结构来推断出隐藏变量（可能是随机变量）的取值，并基于这些值正确推断出反事实的结果。

PS：本文是在用强化学习给因果推断问题建模，之后有时间再细读。

### [Causal Induction from visual observations for goal directed task]()

本文认为虽然数据驱动的方法有很多成功实例，但缺少正确的因果建模是目前泛化问题的主要诱因。本文中，作者提出了一个因果归纳/发现（causal induction）加因果推断（causal inference）的方案来使得智能体可以做因果推理（causal reasoning）。

![image-1](\images\2021-10-18-review of causal model RL\architecture-1.png)

本文考虑的是一个基于目标(goal)的RL环境，其中奖励函数是$r:\mathcal{S}\times \mathcal{A}\times \mathcal{G}\to \mathbb{R}$，策略也是基于目标的：$\pi_G:\mathcal{S}\times\mathcal{G}\to\mathcal{A}$。该文不仅希望学到的策略可以在不同的目标（G）之间泛化，还希望可以在不同的MDP（转移函数）之间泛化。
**但是，本文有一个比较强的假设，就是可以拿到高维图像输入的低维表征-因果宏变量（causal macro-variable），其中包括了cause和effect的宏变量（例如图像包含了电灯开关和照明度，智能体可以直接得到反应开关的状态/cause和照明度状态/effect的变量）。**


在第一步的因果归纳中，本文采用了一种迭代更新因果关系的方式。从无任何因果关系的初始结构开始，不断网上添加新学到的因果关系（类似往一个图上加边）。对每一个观测$o_t$，通过一个编码函数映射成$s_t$，再把这个表征的残差$R=s_{t+1}-s_t$和动作$a_t$拼在一起：$(s_{t+1}-s_t,a_t)$，再讲这个输入一个解码器（edge decoder），生成一个边$\Delta \hat{C}$，从而得到新的因果关系$\hat{C}_{t+1}=\Delta\hat{C}+\hat{C}_t$。其中生成$\Delta \hat{C}$时，是先生成一个注意力向量$\alpha$和一个边权向量$\Delta e$，再内积得到结果（此处对这两个量解释过少，让人难以理解）。从数据中收集完所有因果关系，形成了$\hat{C}_H$后，再把$\hat{C}_H$输入另一个网络生成最终的因果结构。

$$
\hat{C}_{t+1}=(\alpha^T\Delta e)+\hat{C}_t, \alpha=\phi(R,a)
$$


![image-1](\images\2021-10-18-review of causal model RL\model-1.png)

在第二步的策略学习阶段中，先基于当前状态和目标状态生成一个对效用上的概率分布$\alpha$（即为了达到目标，各个效用/effects的似然概率），再基于效用分布和因果关系的内积得到$e$（即为了达到目标，各个动作的似然概率），再将$e$和$(s,g)$的表征一起映射到动作空间。

![image-1](\images\2021-10-18-review of causal model RL\formula-1.png)

![image-1](\images\2021-10-18-review of causal model RL\model-2.png)

这篇文章主要的不足在于并未真正完成声称的“在图像级数据上实现因果归纳和因果推断”，而是借助了宏变量来完成，即在宏变量基础上学习网络结构，而无需把宏变量给学出来。另一方面，值得借鉴的是它在因果归纳中采用了迭代更新的方式，而不是一步到位的因果发现，这或许也是一种可行的思路。

### [RECURRENT INDEPENDENT MECHANISMS]()

本文认为（稀疏的）模块化的网络结构可以让网络获得更好的泛化性能和稳定性。这里的模块化具体是指把神经网络分解成一个个子网络，让输入按指定的（但对不同的输入可能并不固定）顺序通过这些子网络并输出结果。这个想法来自于“模块化的结构下，改动其中一个模块的参数时，可能不会改变其他模块的参数，从而带来一定的稳健性”（换句话说，如果我们在训练中可以只改变一个模块的参数就达到好的训练效果，那么其他模块中之前训练时学到的知识就可以被更好地保留）。这个结构也和因果学习中的局部干预（localized intervention）有相似性（因果学习中认为我对一个变量做干预后，应该只对相邻的变量产生局部的影响，而不是对所有其他变量都产生影响）。

因此本文的主要动机是提出一套方法来从高维观测中学到**倾向于用多个独立的机制的组合**来完成任务的表征。本文提出的网络结构如下：

![image-1](\images\2021-10-18-review of causal model RL\architecture-2.png)

每一个蓝色/白色的方块表示一个循环独立机制（RIM，即一个子网络），对于每一个输入，首先用soft-attention判断输入和每一个RIM的相关性，并只选择相关性高的RIM（蓝色方框）输入。输入后RIM的状态会发生改变，同时产生输出。
此时，不同的被激活的RIMs之间还允许交流，同样是先对每一个RIM，判定有哪些其他RIM的信息是相关的，可以进行交流（指向橙色圆点的黑色实线）。在经过与其他的相关RIM的交流之后，更新当前RIM的状态并产生输出。

实验部分：
1. 复制实验：输入一段序列，紧接着一长段零向量，再要求输出最初的序列。实验表明RIM可以从零向量长度50泛化到200（这也算泛化吗？），都能完成任务。
2. RIM泛化的原理是环境变化时，输入的改变只会激活部分模块，而不是整个网络，所以其他部分依然是稳定的。此时本文做了一个识别MNIST序列任务，并泛化到不同分辨率的环境下。
3. RL环境下，该文章以采用LSTM处理数序列的方法作为baseline，用RIM代替LSTM作为实验组，并用PPO算法学策略。


### [Inductive Biases for Deep Learning of Higher-Level Cognition]()

一个有趣的假设是人类级别的认知能力是可以由几个简单的原则所概括/解释的，如同真实世界是遵循着有简单的物理学规律的。基于这个假设，如果我们可以找到这些最基础的原则（归纳偏置，inductive biases）并让AI掌握这些原则，就可以像人类一样具有很强的泛化能力。过去的研究已经探索了很多归纳偏置，这篇文章主要总结了这些归纳偏置，并讨论了哪些归纳偏置最有利于实现类人的AI性能。

本文的主要假设是，深度学习以往的成功离不开好的归纳偏置，但想要获得好的泛化性能（在OOD的场景下），还需要添加更多/关键的归纳偏置。首先，人类的规律是直觉性的，隐式的；而我们需要的归纳偏置得是明确的，具体的。因此想要引入人类认知的规律必须要先引入更多的关于因果的符号和概念。其中包括我们不能再把数据看作是来自IID的分布，而是应该看成是一个非静态的过程产生的数据。

同时本文对于以往常用的归纳偏置做了总结：

![image-1](\images\2021-10-18-review of causal model RL\table-1.png)


**关于归纳偏置的一些总结**

在算法中引入归纳偏置有很多种形式，包括显示的损失函数、框架性的约束、参数共享、优化方案的选择等等。另一方面，有一些归纳偏置会以数据集的设置（e.g., 数量的多少）的形式出现（e.g., 更多的训练数据可以弥补归纳偏置/先验分布的缺失）。另外，有些归纳偏置带来的好处和改进参数优化过程带来的好处是一致的。

经典的机器学习框架都是基于IID假设的，基于IID我们才可以对泛化误差进行评估和求界。但现实问题往往都不是IID的。为了在变化的环境/任务下学习，我们要关注环境的变化规律/或者做合理的假设：1. 什么是不变的；2. 变化的部分是如何变化的。

**我们可以把学习环境分成两个部分：缓慢学习不变的规律+快速学习变化的部分。这两个部分可能是不同的时间尺度上的学习。**

系统性的泛化（systematic generalization）可能是一种适用于OOD的泛化，因为其可以系统性地从已掌握的概念中衍生出新的概念。

从人的认知角度看，人在新情景下会采用一种非习惯性的认知模式，与旧情景的认知模式完全不同。在认知到新事物后，新事物又会转化成已被认知的旧事物，用习惯性的认知模式去处理。本文整理了一些尚未被充分使用，但可能对OOD泛化有用的归纳偏置。

过去的机器学习方法具有一个归纳偏置：所有的模块/元素总是全程激活/运行的，但实际上，

**面对不同的任务，我们可以只激活网络的部分模块**

同样，每个模块也只选择其关心的信息作为输入。此时，未激活的模块可以视作处于习惯性认知模式（慢学习），而激活的模块视作非习惯的认知模式（快学习）。举个例子，预测台球桌上的移动：未碰撞时处于一种简单的模式，发生碰撞时处于一种复杂的模式。
但是对模块的设置是有无数种可能的，我们此时可以考虑从因果的角度去设置模块。

在因果当中有一种独立机制，即改变其中一个变量，只会导致少数相邻/相关变量的改变，而其他变量都不会改变。因此我们的一种归纳偏置就是

**希望学到的各个模块之间的联系尽可能稀疏，或者说尽可能稳定，不容易受数据的扰动而发生太多改变。**

环境的变化/数据分布的变化可能来自于什么？有两个主要的来源：1.环境的dynamics还未收敛到一个稳态分布（例如玩一个新游戏，玩家尚未掌握技巧，在游玩过程中不断改变自己的玩法）；2.来自智能体的因果干预（例如关闭了一扇不能再打开的门，推箱子游戏中推到了角落）。人类的语言能有非常简短的词语来描述这些改变，这似乎给我们提供了一种归纳偏置：

**在理想的表示空间中，大部分的改变都是局部起效的，即改变只会引起局部的变量/机制发生修改。**

**那些确定依赖关系的独立机制可以是一般/普遍的（generic），即以前是if A， then B；而我们希望是for all X，if f(X), then g(X)。**

人脑可以把感知的信息分割成有含义的事件，并且可以选择性地回忆起很早之前的事件并建立联系。这个带来的归纳偏置是

**因果链可以由时间上相距很远的短因果链串联起来。**
