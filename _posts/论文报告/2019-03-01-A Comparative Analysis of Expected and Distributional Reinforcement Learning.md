---
layout: post
title: A Comparative Analysis of Expected and Distributional Reinforcement Learning é˜…è¯»ç¬”è®°ï¼ˆäºŒï¼‰
date: 2019-03-01 14:37:00
categories: å¼ºåŒ–å­¦ä¹ 
tags: Distributional-RL å¼ºåŒ–å­¦ä¹ ç†è®º AAAI2019
mathjax: true

---

* content
{:toc}

è®ºæ–‡é¢˜ç›®: [A Comparative Analysis of Expected and Distributional Reinforcement Learning](https://arxiv.org/pdf/1901.11084.pdf)

## ç®€ä»‹

è¿™ç¯‡æ–‡ç« æ˜¯åˆ†å¸ƒå‹å¼ºåŒ–å­¦ä¹ (Distributional RL)ç ”ç©¶æ–¹å‘ä¸­ç»¼è¿°å‹çš„ä¸€ç¯‡è®ºæ–‡ã€‚ä¸»è¦è´¡çŒ®ä¸ºä»ç†è®ºå’Œå®éªŒä¸¤ä¸ªè§’åº¦åˆ†æäº†ERL(Expected RL)ä¸DRL(Distributional RL)çš„å¼‚åŒï¼Œå…¶ä¸­åˆ†ç±»è®¨è®ºäº†è¡¨æ ¼å¼æ¨¡å‹ã€çº¿æ€§å€¼å‡½æ•°è¿‘ä¼¼å’Œéçº¿æ€§å€¼å‡½æ•°è¿‘ä¼¼ä¸‰ç§æƒ…æ™¯ï¼Œç»“è®ºä¸ºå‰ä¸¤è€…æƒ…å†µä¸‹ERLä¸DRLæ²¡æœ‰åŒºåˆ«ï¼Œæœ€åä¸€ç§æƒ…å†µä¸‹æœ‰å·®å¼‚ã€‚





## ä½œè€…ä»‹ç»

**ç¬¬ä¸€ä½œè€…ï¼š** Clare Lyleï¼Œç‰›æ´¥å¤§å­¦CS PhDåœ¨è¯»ï¼Œæœ¬ç§‘ä¸ºåŠ æ‹¿å¤§éº¦å‰å°”å¤§å­¦(McGill University)æ•°å­¦è®¡ç®—æœºåŒå­¦ä½ï¼Œè¿™ç¯‡æ–‡ç« æ˜¯å¥¹åœ¨Google Brainæš‘æœŸå®ä¹ æœŸé—´å‘è¡¨ã€‚å¥¹[ä¸ªäººåšå®¢](https://clarelyle.com/index.html)ä¸­æœ‰ä¸€ç¯‡åšæ–‡å¯¹è¿™ç¯‡æ–‡ç« æœ‰è¡¥å……è¯´æ˜ã€‚

**ç¬¬äºŒä½œè€…ï¼š** Pablo Samuel Castroï¼Œéº¦å‰å°”å¤§å­¦CS PhDï¼Œåœ¨Google Brainå·¥ä½œï¼Œä»Šå¹´æ‰å¼€å§‹æœ‰DRLç›¸å…³è®ºæ–‡å‘è¡¨ï¼Œå¦ä¸€ç¯‡ç›¸å…³å·¥ä½œã€ŠDistributional reinforcement learning with linear function approximation ã€‹åœ¨arxivä¸Šå¯æŸ¥åˆ°ã€‚

**ç¬¬ä¸‰ä½œè€…ï¼š** Marc G. Bellemareï¼ŒGoogle Brainçš„ç ”ç©¶ç§‘å­¦å®¶(Reasearch Scientist), éº¦å‰å°”å¤§å­¦çš„å®¢åº§æ•™æˆ(Adjunct Professor), Canada CIFAR AI Chair. ä»–äº2017å¹´å‘è¡¨äº†ã€ŠA distributional perspective on reinforcement learningã€‹ï¼Œæ˜¯DRLçš„å¥ åŸºäººä¹‹ä¸€ã€‚

## DRL å‘å±•å†ç¨‹

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\development.png)

å¯ä»¥çœ‹åˆ°Bellemare(çº¢)ã€Dabney(ç»¿)ã€Munos(è“)æœ€æ—©äº2017å¹´å‘è¡¨ç›¸å…³æ–‡ç« ï¼Œæ˜¯DRLçš„å¥ åŸºäººï¼›ä¹‹åDRLçš„ç›¸å…³æ–‡ç« éƒ½æœ‰è¿™ä¸‰äººçš„èº«å½±ã€‚ï¼ˆDabneyä¸Munosä¸ºDeepMindå‘˜å·¥ï¼‰


## DRL èƒŒæ™¯ä»‹ç»

ERLæ˜¯å°†å¥–èµçœ‹æˆä¸€ä¸ªæ ‡é‡$Q(x,a)$ï¼Œè€ŒDRLæ˜¯å°†å¥–èµçœ‹æˆåˆ†å¸ƒ$Z(x,a)$ï¼Œæ»¡è¶³$Q(x,a)=\mathbb{E}Z(x,a)=\mathbb{E}[\sum_{t=0}^\infty \gamma^tR(x_t,a_t)]$ã€‚ç„¶åä»¥åˆ†å¸ƒçš„å½¢å¼è¿›è¡Œè¿­ä»£ï¼Œä¾‹å¦‚
$$
Z_{i+1}(x_t,a_t)=Z_i(x_t,a_t)+\alpha(R_t+\gamma Z_i(x_{t+1},a_{t+1})-Z_i(x_t,a_t))
$$

æ›´å¤šç»†èŠ‚å¯ä»¥å‚è€ƒç¬”è€…ä¹‹å‰çš„å¦ä¸€ç¯‡å…³äºDRLçš„[é˜…è¯»ç¬”è®°](https://siqili1230.github.io/2019/01/03/Implicit-Quantile-Networks-for-Distributional-Reinforcement-Learning/)ã€‚

é‚£ä¹ˆä»¥åˆ†å¸ƒçš„å½¢å¼ç ”ç©¶RLæœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Ÿ

ç›®å‰æ™®éè®¤ä¸ºï¼ˆå¹¶éè¯å®ï¼‰æœ‰ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢çš„æ„ä¹‰ï¼š
1. é™ä½æ–¹å·®ï¼šä»¥åˆ†å¸ƒçš„å½¢å¼é¢„æµ‹æœªæ¥çš„å›æŠ¥ï¼Œè¢«è®¤ä¸ºèƒ½é™ä½é¢„æµ‹å›æŠ¥çš„æ–¹å·®ã€‚
2. æ›´å¥½çš„ä¼˜åŒ–è¡¨ç°ï¼šåˆ†å¸ƒæˆ–è®¸èƒ½ä½œä¸ºä¸€ä¸ªæ›´å¥½æ›´ç¨³å®šçš„ä¼˜åŒ–ç›®æ ‡ï¼Œåœ¨æŸäº›ç¥ç»ç½‘ç»œä¸­æˆ–è®¸èƒ½æœ‰æ­£åˆ™åŒ–çš„æ•ˆæœã€‚
3. è¾…åŠ©ä»»åŠ¡ï¼šåˆ†å¸ƒèƒ½æä¾›æ›´ä¸°å¯Œçš„é¢„æµ‹ä¿¡æ¯ç”¨äºå­¦ä¹ ã€‚

åŸºäºåˆ†å¸ƒè¿›è¡Œç ”ç©¶çš„å¿…è¦å·¥å…·ï¼š

#### **1 åˆ†å¸ƒçš„è·ç¦»**

åœ¨ã€ŠImplicit Quantile Networks for Distributional Reinforcement Learningã€‹ä¸€æ–‡ä¸­æåˆ°Wassersteinåº¦é‡æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åˆ†å¸ƒåº¦é‡ï¼Œä½†å…¶åœ¨å®é™…ä½¿ç”¨ä¸­éš¾ä»¥è®¡ç®—å’Œåˆ†æï¼Œå› æ­¤æœ¬æ–‡ä¸­ä½œè€…é‡‡ç”¨Crameråº¦é‡ï¼š

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\def_1.png)

#### **2 åˆ†å¸ƒçš„è¡¨ç¤º**

ç”¨ç‚¹æ”¯æ’‘é›†è¡¨ç¤ºï¼Œè®°$\mathbf{z}=\{z_1,\cdots,z_k\}$ï¼Œå…¶ä¸­$z_1 \leq z_2 \leq \cdots \leq z_k$ï¼Œåˆ™ä¸€ä¸ªç”¨æ”¯æ’‘é›†$\mathbf{z}$ è¡¨ç¤ºçš„åˆ†å¸ƒ$P$å¯ä»¥å†™æˆï¼š

$$
P\in Z_z:=\{\sum_{i=1}^{k}\alpha_i\delta_{z_i}:\alpha_i\geq 0,\sum_{i=1}^{k}\alpha_i=1\}
$$

é‚£ä¹ˆCrameråº¦é‡å¯ä»¥é‡æ–°å†™ä¸ºï¼š

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\cramer_1.png)

## CrameræŠ•å°„

ä¸ºäº†è§£å†³è¿­ä»£è¿‡ç¨‹ä¸­æ”¯æ’‘é›†å˜åŒ–çš„é—®é¢˜ï¼Œå³$Z_{i+1}(x,a)$ä¸$Z_i(x,a)$çš„æ”¯æ’‘é›†ä¸ä¸€è‡´ï¼Œæå‡ºäº†Cramer Projectionæ–¹æ³•:

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\def_2.png)

å¦å¤–Cramer Projectionæœ‰ä¸€ä¸ªæ€§è´¨ï¼š

$$
\mathbb{E}[\Pi_C(P)]=\mathbb{E}[P]
$$

å³å…³äºæœŸæœ›è¿ç®—ä¸å˜ã€‚

## æœŸæœ›ç­‰ä»·æ€§(expectation-equivalent)

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\update_1.png)

é¦–å…ˆï¼ŒERLä¸DRLçš„æ€§èƒ½æ¯”è¾ƒæ—¶ï¼Œè¦é‡‡ç”¨æˆå¯¹çš„ç®—æ³•ï¼Œæ¯”å¦‚ERLç”¨bellmanç®—å­ï¼ŒDRLä¹Ÿè¦ç”¨ç›¸ä¼¼çš„åˆ†å¸ƒå‹çš„bellmanç®—å­ã€‚

å…¶æ¬¡ï¼Œè¿™é‡Œå®šä¹‰æœŸæœ›ç­‰ä»·æ€§ã€‚

è®°

$$
Z\overset{\mathbb{E}}= Q \Longleftrightarrow \mathbb{E}[Z(x,a)]=Q(x,a)  \ \ \ \forall (x,a)\in \mathcal{X}\times \mathcal{A}
$$

æˆ‘ä»¬ç§°ä¸¤ç§æ›´æ–°è§„åˆ™$U_E$å’Œ$U_D$æ˜¯æœŸæœ›ç­‰ä»·çš„ï¼Œå¦‚æœæœ‰ä¸‹å¼æ»¡è¶³ï¼š

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\update_2.png)

åœ¨æ»¡è¶³æœŸæœ›ç­‰ä»·æ€§çš„æˆå¯¹æ›´æ–°è§„åˆ™ä¸‹ï¼Œä»¥ä¸‹ä¸‰ç‚¹å…³äºDRLå¯èƒ½å­˜åœ¨çš„å‡è®¾å‡è¢«æ¨ç¿»ï¼š

1. DRLå¯ä»¥é™ä½æ–¹å·®ã€‚
        å› ä¸º$ğ‘‰ar[\mathbb{E} Z_t (x,a)]=Var[Q_ğ‘¡ (ğ‘¥,ğ‘)], âˆ€ (ğ‘¥,ğ‘)$ 
2. DRLæœ‰åˆ©äºç­–ç•¥è¿­ä»£ã€‚
        å› ä¸ºè´ªå¿ƒç­–ç•¥åŸºäº$ \argâ¡max[â¡ğ‘„_ğ‘¡ (ğ‘¥,â‹…)]  $å’Œ$\argâ¡maxâ¡[\mathbb{E} ğ‘_ğ‘¡ (ğ‘¥,â‹…)]$,DRLåœ¨æ‰€æœ‰åŸºäºæœŸæœ›åšå†³ç­–çš„ç­–ç•¥ä¸­éƒ½æ²¡æœ‰å¸®åŠ©ã€‚
3. DRLåœ¨å€¼å‡½æ•°è¿‘ä¼¼ä¸­æ›´ä¸ºç¨³å®šã€‚
        ä¸‹æ–‡ä¼šè¯´æ˜ï¼Œåœ¨çº¿æ€§å€¼å‡½æ•°è¿‘ä¼¼æƒ…å†µä¸‹ï¼ŒDRLæ²¡æœ‰å¸®åŠ©ã€‚


## ç†è®ºåˆ†æ

### tabular model

1. model-based

åŸºäºæ¨¡å‹çš„æƒ…å†µï¼Œå³å¯ä»¥åˆ©ç”¨åˆ°çŠ¶æ€è½¬ç§»çš„ä¿¡æ¯ã€‚

é¦–å…ˆè€ƒè™‘$Z$ä¸ºä¸€èˆ¬çš„åˆ†å¸ƒï¼Œå³æ”¯æ’‘é›†æ˜¯å®æ•°åŸŸã€‚

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_2.png) 

ä¹‹åè€ƒè™‘$Z$ä¸ºDiracå‡½æ•°ï¼Œå³ç‚¹æ”¯æ’‘é›†ã€‚

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_3.png) 

2. sample-based

åŸºäºé‡‡æ ·çš„æƒ…å†µï¼Œå³åªèƒ½æ ¹æ®é‡‡æ ·å¾—åˆ°çš„å›ºå®šè½¨è¿¹è¿­ä»£ä¼˜åŒ–ã€‚

ä¾ç„¶å…ˆè€ƒè™‘$Z$ä¸ºä¸€èˆ¬çš„åˆ†å¸ƒã€‚

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_4.png) 

ä¹‹åè€ƒè™‘$Z$ä¸ºDiracå‡½æ•°ã€‚

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_5.png) 

**semi-gradient**

ä¹‹å‰éƒ½æ˜¯åŸºäºbellmanè¿­ä»£çš„ï¼Œç°åœ¨è€ƒè™‘åŠæ¢¯åº¦æ–¹æ³•ã€‚

é¦–å…ˆå®šä¹‰Crameråº¦é‡ä¸‹çš„æ¢¯åº¦ï¼Œå¯ä»¥åˆ†ä¸ºå¯¹CDFæ±‚æ¢¯åº¦å’Œå¯¹PDFæ±‚æ¢¯åº¦ã€‚

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\grad_1.png) 

ä»¥ä¸‹åˆ†åˆ«æ˜¯ç”¨CDFå’ŒPDFæ±‚æ¢¯åº¦çš„ç»“è®ºã€‚

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_6.png) 

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_7.png) 

### å‡½æ•°çº¿æ€§è¿‘ä¼¼

é¦–å…ˆç”¨çº¿æ€§å‡½æ•°è¿‘ä¼¼åˆ†å¸ƒå‡½æ•°ï¼Œä¸ERLä¸­çš„$Q$å€¼å¯¹æ¯”å¦‚ä¸‹ï¼š

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_8_1.png) 

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_8_2.png) 

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_8.png) 

### å‡½æ•°éçº¿æ€§è¿‘ä¼¼

![](\images\2019-03-01-A Comparative Analysis of Expected and Distributional Reinforcement Learning\prop_9.png) 

## å°ç»“

è™½ç„¶ç»“è®ºéå¸¸ç®€æ´æ˜äº†ï¼Œè¯æ˜è¿‡ç¨‹ä¹ŸåŸºæœ¬æ²¡æœ‰é—®é¢˜ï¼Œä½†è¿™ç¯‡æ–‡ç« æœ€ä¸»è¦çš„é—®é¢˜åœ¨äºæ–¹å‘æœ‰åé¢‡ã€‚ç§ä»¥ä¸ºæ—¢ç„¶DRLçš„ç‰¹ç‚¹åœ¨äºç”¨åˆ†å¸ƒçš„è§’åº¦çœ‹å¾…å›æŠ¥ï¼Œé‚£å°±ä¸åº”è¯¥ç”¨å›æŠ¥çš„æœŸæœ›å»åšå†³ç­–ï¼Œè€Œæ˜¯åº”è¯¥å……åˆ†åˆ©ç”¨å›æŠ¥çš„åˆ†å¸ƒæ¥åšå†³ç­–ï¼Œè¿™æ‰æ˜¯DRLé¢†åŸŸæœ€é‡è¦çš„å‡ºè·¯ã€‚





